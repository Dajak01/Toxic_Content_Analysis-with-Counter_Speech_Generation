{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baacc7a-547f-42c0-afb1-1381ebe3b41a",
   "metadata": {},
   "source": [
    "### Counter Speech Generation with GPT-2, EAR, and KLAR Regularization with LOTO Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7c792d-3215-4aec-a1b2-4a46c60486cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load additional hate speech and counter-narrative dataset\n",
    "cn_data = pd.read_csv('MT-CONAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8400c838-b26e-4057-9c97-4801b03e1542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>HATE_SPEECH</th>\n",
       "      <th>COUNTER_NARRATIVE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>VERSION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Maybe the UN could talk to those asian and afr...</td>\n",
       "      <td>You're right, ocean pollution is one of the ma...</td>\n",
       "      <td>other</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>‘Juice (Jews) are worse that nukes.’ Caption t...</td>\n",
       "      <td>You'd rather be nuked then?</td>\n",
       "      <td>JEWS</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Foreigners on UK benefits leaps 41% in 5 years...</td>\n",
       "      <td>You seem to be worried about government expend...</td>\n",
       "      <td>MIGRANTS</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jews cannot be patriots, since their allegianc...</td>\n",
       "      <td>You can have parents and grandparents born els...</td>\n",
       "      <td>JEWS</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Israel, Zionism and Jewish Supremacist control...</td>\n",
       "      <td>Wouldn't you say its colonialism from many dif...</td>\n",
       "      <td>JEWS</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDEX                                        HATE_SPEECH  \\\n",
       "0      0  Maybe the UN could talk to those asian and afr...   \n",
       "1      1  ‘Juice (Jews) are worse that nukes.’ Caption t...   \n",
       "2      2  Foreigners on UK benefits leaps 41% in 5 years...   \n",
       "3      3  Jews cannot be patriots, since their allegianc...   \n",
       "4      4  Israel, Zionism and Jewish Supremacist control...   \n",
       "\n",
       "                                   COUNTER_NARRATIVE    TARGET VERSION  \n",
       "0  You're right, ocean pollution is one of the ma...     other      V1  \n",
       "1                        You'd rather be nuked then?      JEWS      V1  \n",
       "2  You seem to be worried about government expend...  MIGRANTS      V1  \n",
       "3  You can have parents and grandparents born els...      JEWS      V1  \n",
       "4  Wouldn't you say its colonialism from many dif...      JEWS      V1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee41de6-a1b7-4057-bcb2-c3d0b585c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2501.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1444.386029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1250.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2501.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3751.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             INDEX\n",
       "count  5003.000000\n",
       "mean   2501.000000\n",
       "std    1444.386029\n",
       "min       0.000000\n",
       "25%    1250.500000\n",
       "50%    2501.000000\n",
       "75%    3751.500000\n",
       "max    5002.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d737e2e-7021-4293-8552-e60a4d154ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique groups in the TARGET column are:\n",
      "['other' 'JEWS' 'MIGRANTS' 'WOMEN' 'POC' 'LGBT+' 'MUSLIMS' 'DISABLED']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "cn_data = pd.read_csv('MT-CONAN.csv')\n",
    "\n",
    "# Get unique groups\n",
    "unique_groups = cn_data['TARGET'].unique()\n",
    "\n",
    "# Display unique groups\n",
    "print(f'The unique groups in the TARGET column are:\\n{unique_groups}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976052ca-d163-46d5-932a-52e8d3b146c6",
   "metadata": {},
   "source": [
    "### Model Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2dcd0a-e079-447d-b1b5-029c1042fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1794' max='1794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1794/1794 12:07:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-14.102500</td>\n",
       "      <td>-19.272741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-18.432300</td>\n",
       "      <td>-19.792513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-18.775800</td>\n",
       "      <td>-19.954700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1656' max='1656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1656/1656 6:58:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-19.240900</td>\n",
       "      <td>-18.852114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.401200</td>\n",
       "      <td>-18.972481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.738900</td>\n",
       "      <td>-19.008690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1647' max='1647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1647/1647 10:15:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-19.570400</td>\n",
       "      <td>-19.938175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.475200</td>\n",
       "      <td>-19.961380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.859900</td>\n",
       "      <td>-19.995350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1518/1518 6:11:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-20.132500</td>\n",
       "      <td>-18.019915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-20.212600</td>\n",
       "      <td>-18.027205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-20.185700</td>\n",
       "      <td>-18.108946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 20:23:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>-19.465967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.985300</td>\n",
       "      <td>-19.466116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.943400</td>\n",
       "      <td>-19.461040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1746' max='1746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1746/1746 15:31:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-19.748600</td>\n",
       "      <td>-20.621311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.940600</td>\n",
       "      <td>-20.648624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.835300</td>\n",
       "      <td>-20.664507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1629' max='1629' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1629/1629 11:06:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-19.492600</td>\n",
       "      <td>-22.274693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.711800</td>\n",
       "      <td>-22.274582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.642800</td>\n",
       "      <td>-22.294003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1779' max='1779' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1779/1779 16:59:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-19.915900</td>\n",
       "      <td>-22.436157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-19.843300</td>\n",
       "      <td>-22.432720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-19.554700</td>\n",
       "      <td>-22.450878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, GPT2Config\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn  \n",
    "\n",
    "# Load the data\n",
    "cn_data = pd.read_csv('MT-CONAN.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "cn_data.fillna(\"\", inplace=True)\n",
    "texts = cn_data['HATE_SPEECH'].tolist()\n",
    "targets = cn_data['TARGET'].tolist()\n",
    "counter_texts = cn_data['COUNTER_NARRATIVE'].tolist()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {'additional_special_tokens': ['<hatespeech>', '<counternarrative>']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "class CounterSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, counter_texts):\n",
    "        self.texts = texts\n",
    "        self.counter_texts = counter_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = f\"<hatespeech> {self.texts[idx]} <counternarrative>\"\n",
    "        target_text = self.counter_texts[idx]\n",
    "\n",
    "        encodings = tokenizer(input_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
    "        counter_encodings = tokenizer(target_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encodings.items()}\n",
    "        item['labels'] = counter_encodings['input_ids'].squeeze()\n",
    "        return item\n",
    "\n",
    "# Define the model with regularization and dropout\n",
    "class GPT2WithRegularization(GPT2LMHeadModel):\n",
    "    def __init__(self, config, dropout_rate=0.5):\n",
    "        super().__init__(config)\n",
    "        self.ear_weight = 0.1\n",
    "        self.klar_weight = 0.1\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Adding dropout layer\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        outputs = self.transformer(input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   position_ids=position_ids,\n",
    "                                   head_mask=head_mask,\n",
    "                                   inputs_embeds=inputs_embeds,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states,\n",
    "                                   return_dict=return_dict)\n",
    "        \n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.dropout(hidden_states)  # Apply dropout to hidden states\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            # EAR\n",
    "            entropy = -torch.sum(F.log_softmax(lm_logits, dim=-1) * F.softmax(lm_logits, dim=-1), dim=-1).mean()\n",
    "            loss += self.ear_weight * entropy\n",
    "\n",
    "            # KLAR\n",
    "            attentions = outputs.attentions\n",
    "            if attentions is not None:\n",
    "                for attention in attentions:\n",
    "                    attention = attention.mean(dim=1)\n",
    "                    target_attention = torch.eye(attention.size(-1)).to(attention.device)\n",
    "                    klar = F.kl_div(attention, target_attention, reduction='batchmean')\n",
    "                    loss += self.klar_weight * klar\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": lm_logits, \"hidden_states\": outputs.hidden_states, \"attentions\": outputs.attentions}\n",
    "\n",
    "# Enable attention outputs in the model config\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.output_attentions = True\n",
    "model = GPT2WithRegularization(config)\n",
    "\n",
    "# Update model config for additional special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_ratio=0.1,  # Using warmup ratio instead of warmup steps\n",
    "    learning_rate=5e-5,  # Set learning rate to 5e-5\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=False,  # Disable FP16 as we are not using CUDA/GPU\n",
    ")\n",
    "\n",
    "# LOTO Evaluation\n",
    "logo = LeaveOneGroupOut()\n",
    "for train_idx, val_idx in logo.split(texts, counter_texts, groups=targets):\n",
    "    train_texts = [texts[i] for i in train_idx]\n",
    "    val_texts = [texts[i] for i in val_idx]\n",
    "    train_counter_texts = [counter_texts[i] for i in train_idx]\n",
    "    val_counter_texts = [counter_texts[i] for i in val_idx]\n",
    "\n",
    "    train_dataset = CounterSpeechDataset(train_texts, train_counter_texts)\n",
    "    val_dataset = CounterSpeechDataset(val_texts, val_counter_texts)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb810d5-f34c-4089-b994-3faec16fe00b",
   "metadata": {},
   "source": [
    "### Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008a5364-9a10-4ab8-bbf6-03c2af7c0517",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./results does not appear to have a file named config.json. Checkout 'https://huggingface.co/./results/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: lm_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs\u001b[38;5;241m.\u001b[39mhidden_states, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs\u001b[38;5;241m.\u001b[39mattentions}\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load the model configuration with attention outputs enabled\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2WithRegularization\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py:546\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    548\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    549\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:380\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m         )\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: ./results does not appear to have a file named config.json. Checkout 'https://huggingface.co/./results/None' for available files."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the custom GPT2 model with regularization and dropout\n",
    "class GPT2WithRegularization(GPT2LMHeadModel):\n",
    "    def __init__(self, config, dropout_rate=0.5):\n",
    "        super().__init__(config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Adding dropout layer\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        outputs = self.transformer(input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   position_ids=position_ids,\n",
    "                                   head_mask=head_mask,\n",
    "                                   inputs_embeds=inputs_embeds,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states,\n",
    "                                   return_dict=return_dict)\n",
    "        \n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.dropout(hidden_states)  # Apply dropout to hidden states\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": lm_logits, \"hidden_states\": outputs.hidden_states, \"attentions\": outputs.attentions}\n",
    "\n",
    "# Load the model configuration with attention outputs enabled\n",
    "config = GPT2Config.from_pretrained('./results', output_attentions=True)\n",
    "model = GPT2WithRegularization.from_pretrained('./results', config=config)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add the special tokens\n",
    "special_tokens = {'additional_special_tokens': ['<hatespeech>', '<counternarrative>']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Inference functions remain the same\n",
    "def generate_beam_search(prompt, model, tokenizer, num_beams=3, max_length=50):\n",
    "    input_text = f\"<hatespeech> {prompt} <counternarrative>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_top_k_sampling(prompt, model, tokenizer, top_k=50, max_length=50):\n",
    "    input_text = f\"<hatespeech> {prompt} <counternarrative>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    top_k_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    return tokenizer.decode(top_k_output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_top_p_sampling(prompt, model, tokenizer, top_p=0.9, max_length=50):\n",
    "    input_text = f\"<hatespeech> {prompt} <counternarrative>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    top_p_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return tokenizer.decode(top_p_output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_top_k_top_p_sampling(prompt, model, tokenizer, top_k=50, top_p=0.9, max_length=50):\n",
    "    input_text = f\"<hatespeech> {prompt} <counternarrative>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    top_k_top_p_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return tokenizer.decode(top_k_top_p_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Usage\n",
    "prompt = \"I hate this!\"\n",
    "print(\"Beam Search Output:\", generate_beam_search(prompt, model, tokenizer))\n",
    "print(\"Top-k Sampling Output:\", generate_top_k_sampling(prompt, model, tokenizer))\n",
    "print(\"Top-p Sampling Output:\", generate_top_p_sampling(prompt, model, tokenizer))\n",
    "print(\"Top-k and Top-p Sampling Output:\", generate_top_k_top_p_sampling(prompt, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7885af-ea99-4f3f-97c1-45e8479bfe29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
